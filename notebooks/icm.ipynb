{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Networks (Siamese, Action Embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add the src folder to sys.path\n",
    "sys.path.append(str(Path().resolve().parent / \"src\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random\n",
    "from torchvision.transforms import functional as F\n",
    "from PIL import Image\n",
    "from custom_dataset import SasDataset\n",
    "from siamese import EncoderModel, InverseModel\n",
    "from action_embeddings import ForwardModel, EmbeddingModel\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, random_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display random Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here the random image path from the folder:\n"
     ]
    },
    {
     "data": {
      "image/jpeg": "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",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAFAAAABQCAIAAAABc2X6AAAPZElEQVR4Ae1b2XMUxxnfOfaQVoeFkE24DOYSBQJsDBjH2LFjx2fiIxU78UOq8uK3VOU1iV9SeUn+gDyk8piqHJXYxikfpIiNsTkMNkYxhzFgI3FYgJCQBCvt7M6R39c9M9sz07070opKymHYGvV8/fu+3/d19/R80z1oDz/5e9d1L40M45z5+h66rt/W3YOzuaL3uXLF2n/qNcuyMpo2uyF7mYznOLCpGwbOLitrhsFppLVSZLNeeV4+n9+85buFXN50nAp+rlvFb9YDNnR96eLb4e6XFwZxXs7KZy4MOmw0SWulyOYDdl2dRaqZzdqqr69phXwBEN6lvFxrVlmtHFmfZTq16oBp+GHQNTy0DBuuEVyg6zj20WP9VGXSkKYyAsrlSZLJOI5z9Hg/kYS1kIZlAcng7BRYrknkJZlXDBkN2GURaplsNnvvxq04e169mDVNq1ar+/79Ec5+v3leHd0IPlOPJYLkM0tdy2LUEl2hOggYcXmZtq4iqq5fvY7ZbNmiJRiBnldv6tY0vWyVPzxySDCYqaMbw6dHcvt18KIDMRaxCmU/YE3XFq1eMO/2Hs/1Ri5eHTp52cZwdOxGPewCFrOIS5WupsXx6ZGcRYUXfUiyiLUsYM8zc+atC7tdx7WrzoJl84YHRvWMhhalgL2My/pZ1yATdTGK0VB6RCRcpMED47MIimCkf7JDhRc9rO9VbUjD90q5iojKJQt8E6Vrll1BwNAvtrSCfWLyWqzDUWVVCJP0DcJrk9djVTE8aomlaiVhba10c8WOOnjRwxhLzAgLGHNPxb5w+mI2Z3LnLavy6jtvogd1TcMj+/nvPA217bveRnLiMgS6BZJachbNWEBZqVYb4oEhFqQlgrWQEUYivgY2o1JMPV5O8LDC0if0APkWs8DMBT2c8YZODYUI3TRum3srAJdHhvHw4MQooI2RoJF89ArOvDw8eoW3AoeF54Z4xNYzZ27MmsgYmgoLPFcLL6ngIZereUhlHqcsWsDDgDMZA7coaz7PQ5Mvmb8Y1VeujpDR4KjJx0Yh45iRsVHXtsPGCrD0tx6e5Zs+i2Atxihao7IsjHAsUIH/4mq1az9gTMgZ7jRCNkxMhgf6DxBKN0zTx8Ac2s+XGyQ88PE+wuRyKj9keA/2SYtN5gc+3ktpWJ5SkSQjQ0VP8BNHYCFaJ7sCHuPb82yWzwCBm9YzDWPTmjv5YLBt+8ip4zj7RpMTEieDlmmu27AJJj49eYzwsrYnFwR83/q7WfOJk5zWmJGskNPEuHodig0YGZzj+3r7iNHzDMNAmBQsag3duLO3Dw2NPiyXreNffk4B8BuDTSfcgn8O5DBBWpnMsS9OEF51RPGFQh6sITYVY4BOyxjF+4wesgNyMhxddAF6nmygVTb33QXJwSOfhP5R80Tl3ERgP/5XirdtauYQWp8xhIWF+owhLCwAH2P0Aw4RKMAJDO+TA1+gjALaldeq5KJuiEQhiYcEh4jnEpxjjDGYqNJkWRIwLKITrrAHD7wWCVRyEYMyHrBcEsNDjnk71sMcnETGbM7WpTxgsh4NtcanknMETw/efQtXFAnAAR6XSEUi7cdUML4lSNFCjXsWSvKAMaLaim0wf710XSRRyUUM+mpyapIkQai8FvIpLhfRvJxASi0k9WYgkaT+8Az37V2r1+GHAi65XZVcwip0bKSWy5PnCIhdqCwkkdOUmKbwEEcHmoaDM6a23Qf3kCmWS3NMUs65aObEDy7iEKzxWqqa1qGygOTB8afPpD0xCrEWcvzCPkOViaeugNCqeD/k67X8CYxM1XVPnDkFTEzOtbAQ17tkBbTQHI7jhgt0Ye3SRSsMAxOVQKIoosXqWvCyZhZ0vhuCkbiHQRXJB05BC7NwIMuYuw98EF6QHP2ENiZy1jNIM217L8cgAQzlOjU2Wg5P5vs3bg2TlnOXLjjhcm9Q21LI67q/NFvjipbA7LrOVNmSWgiTB8d1bNgXDzYe9x5iSS73nNcy+b5D+/2gAhVTMgiZo2t71wJz5NRnOPet31grc/nJ4+E44ckADXjZ6IWwbCEb+5ylrgFt7C/ya8NYs2yVykKYPIAlpupfJm8EXpGQy2ZprJF43tWJcah4bHj7ZTYux7icLQzIuaNS1hDOR8cOO4hb5S6myUJ+1ZLlyniiNpu5osSa9KOu4O158NwZGgwszRo8e8Yf6pnMwLkBwrM7HP6FLoplAkQPzByOGbypRqvoCmMq0RUhSrQslkMAtxC5xEU0ohBjPvvwk5VqZef+3ZVKxQdhDSGXe2TbwzjzcQsa1O78kGGCTBPZwhvv78RLfMiEceGnEKwNQ/nMCyyNUbJwu9zbex6Qe8vd45itD+SwpjOvu2eqYiEk0S1cYi2iJe+/2eByyopj0BYjPP0UNdHztp4v2BVHxwBpPnAli0CaxlsfQ3tLGL78OSSYQBFCVIU9LMXwgS3q6Z72xKaLP952/qur+fdPdP3zyO2IOdKWIjplOfmKmlBM4y3HSDKthDW1wHXotTn82W5ba/XXz39+4mJxQVe5o2iXymaz0YI8xoLLJg7ZLJ3OHC3B9czD248/cOnJ7VUrQ2bFvfRZrqXQ+ujcy6/od3g0Mcx8aCdZ0FGqZcM0js80YOz05HKP3/ftQuQ+r7yy4+9vvDX30EDnB+Pd95nD+kzN+67LWMqW9ee3X53xbnZTHqGxcYT3uec6FU/7+d41GaRV45lPvfZci867lzKKOutewiJbspdiLLhMYtJLmgo4SUN3rIFRzGYq9hfNkTXNe9dvYpmW4o4OFtmqddbGkmQzksxCwFgNpMc8vWlJXoxQi72L9SvXIHIC0ZSbvKUxDugthQ+WWCCwELzxECZWO93LpgKG/+iT9/oP2mxxA95QF4WPdFb7zsE9CLGtq63Y3lKamLqGvVis+CsOlQW8JFE2ls1+c8NmkCq0U4mbChgMuKO+ODfgWGUWZ3zfHbWDg192fqMr15otdGZdzR4bGR8bHKFdDsnBntlBJsfryQIls0hiPCNf2LrubonedETNBgwuDDlHfC+L0Wt697w5Hbe0Y3WrY06HVaqMXbgq+UoipiVe8iagfJveSZs8lAGLO7F068n2gaVCcgj7yeF96nlVDHTbtqYquUK2UsHeB3acU9yKNPPFB4KKMY23vKXkAWPyEHduEXByHxiY8dI1io6/b3F7bPoqtrbX7jTPK49aJXMKmWpVt91Jt72jM4OnVaMDZktTk6JxFSPkDb0N2bSXfnYBH6b9dcf22KMcKZToFDos+QCkNCsmZ5+A/eCR72FmrvmKL0X45zLUHPRlQUivKlATW9bfdv4j6VWckZlo4C3z6oXHnqEP01SUkp3YxPToYxJyLF/gfqsFrOJQyxGwLbtjVYxpvOVsyoBrTxe1WyoMQuVHHdWGVbAgwSQa18eo5AkTfsDy1A9jOrkQgSRR9ESKYTS0iCHeFQlupcDDCqnilSgNewwDGiF1je0Piz4IO7e8/djinr/j6gedwDADNCAd55PPPsXkjNtWNJqizPaBV66pTXtcJw27BMOU2SIhv8uoh8P9YdEb8NX2ioMK3JzYEw73eKUYjkXmfPjEkXoLd4HN+F+2oLdmWW9cTutrjdljmJoR6f5wrZo9XdIsmkox3E6DhTuRTCxTgqGcWUAnLtlK2UWMaNj3iv+Jjx8WcFIIMIRSuWhaCeDzkDjBJCXMkNKCUEt+iKYCD1RyPgv6bUmrjdEDakkhIBDioceVc1nsYkgOqSJw+O4UZ/EdMCnh5lQWxNo0HnI8P+MtFQVKPKxq5fVdO/BJF599ebPRcyXjxRIsVOGLU5qF2Odgzzz4OFB/2RH5mh6YFvaN9JRV5u1CfGyh9OkHH0fx9V1v05Iwff2Ti0iC7lJZQDLzw8eeRS32mcMP0PB1a42FDUDfQ2INDubt0w8+lscyLWRQQBJXtawi2xMuTZYgLCo+/vP3eDHRu/SdWmCy9hdC6e4uHOUfCKLA0UkJl6ssiLU8YJIE1sJayS604K0/pOEC3jbvXrMBansP0xdavLyv/2DtW2huMiAI/ebiyDnARISsZRtKfIDCAq8lav6LmQuqk+LQ2yBgttfm7wmzXHf3/vdITfyCVj15xgnw6I8daGP17i5Ntvglg2yGUaHrB+y7F6T19N8v7lgV7uvCk+TObSyi8JJ0JXvCyt1d4PkOM1o3NNIMY33daMCckOUr2PWlBIO95eBtGElIZOc2dC1WSOj69YiFJYyx3V3cruEOs5ixNsNYX1cWMPNRfHxrWJtJjtJYqMKlqCuI6RkuXoblpPFmGOvrNn41Dd36ehRMtLq04UW5WE4TtgqflCcl3L5KrmIX8WJZxHO5+eq/3qzt6wZsSHQa7MqKlsSybEdXrBe5kiwqpCiPl2WMIgvhOWY37WabwyPDJArmZyqzB6Z875dX1z1jHpLoiioBV3qkqJ0sy+0ELBwfYsxYqDVzUYWaPE0pvW56ZH3eNHYYxqTd3RkctDPiK1IBRhQz8Axsy1VmidFctHB5jAC9T+8NtEqAGvmDBNVZfDCRKwKxeP4d1UrwX/FitmbxUs6IdicOtpOhcDXqg7blvl9EJZmWFj2X01tbjaGh6CdgMVywQMsXaxOVN0TAl4pDRkxO8zvof69+NVEWP6+ZBjea6vvPdf72N4u3b199/7aWaWj+N6BPrep476UN+D2xsj0lfzzxwAjpntuR0Vpefvn8Rx836OGUHDcI1lEwn79raT5fLLa0/fT+3jb1GrvoQDxg1E1M2GvXtj31VM/SpfQ/8P5nD+xPvXtq5OzVqWLOODtWtvnd3MhdSS69fful1lZ97dr2NDtejezfwPrrFXvPmdEVPcXz41O/2zdYtlNs0N1AdxSmDWHHipfxODMNTZQrVGdHnGoqb4ZqxcK2nzy6+Fd/PDF/TmFbX/dYyZ7XlT98euye1XMGLk3O7czvOzayfH4xa+pzO3Ov7Rl6Ysttt7Rl9xwd7T89JltBasYX0pXcw82ajOqPXa/2Le3AkgxCome7rlWxFkb/Dw+PUO+Fby24PFYZL9kArF3S8aOHFs7rKvzpnfO/fHFle4t8STRqftpXNzZgjNUNyzovj1lbVndt7u3auOqWcsU5cfZa/+nxowMThZyxq3+4pzPX3kq7UEMj5Tc+vDhp2S8+tPAPbw6Uyol1omlHJ1G4sUMa92exYFZtF907VXGyBvUzVjZKZae1QLftxKQNSUtOR59fm6QIodKaNwCQOHtTdLMFbrbAzRb4v2uB/wB6MNwGXOHXXQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=80x80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displayed image: ../data/states/state_1735213548910_obs_t.png\n"
     ]
    }
   ],
   "source": [
    "# https://www.geeksforgeeks.org/show-random-picture-from-a-folder-in-python/\n",
    "def get_random_image_path(folder_path):\n",
    "    try:\n",
    "        files = os.listdir(folder_path)\n",
    "        \n",
    "        images = [\n",
    "            file\n",
    "            for file in files\n",
    "            if file.lower().endswith((\".png\", \".jpg\", \".jpeg\", \".gif\"))\n",
    "        ]\n",
    "\n",
    "        if not images:\n",
    "            print(\"No images found in the specified folder.\")\n",
    "            return None\n",
    "\n",
    "        \n",
    "        random_image = random.choice(images)\n",
    "        random_image_path = os.path.join(folder_path, random_image)\n",
    "        print(f\"Here the random image path from the folder:\")\n",
    "        return random_image_path\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while selecting image randomly: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def display_image(image_path):\n",
    "    try:\n",
    "        if image_path and os.path.isfile(image_path):\n",
    "            with Image.open(image_path) as img:\n",
    "                img.show()\n",
    "                print(f\"Displayed image: {image_path}\")\n",
    "        else:\n",
    "            print(f\"Invalid image path: {image_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while displaying the image: {e}\")\n",
    "        \n",
    "def show_random_image_from_folder(folder_path):\n",
    "    random_image_path = get_random_image_path(folder_path)\n",
    "    display_image(random_image_path)\n",
    "\n",
    "\n",
    "folder_path = '../data/states'\n",
    "if os.path.isdir(folder_path):\n",
    "    show_random_image_from_folder(folder_path)\n",
    "else:\n",
    "    print(f\" The specified folder does not exist: {folder_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid actions:\n",
      "                                               state  action  \\\n",
      "271               ../data/states/state_271_obs_t.png       5   \n",
      "1749             ../data/states/state_1749_obs_t.png       7   \n",
      "3049             ../data/states/state_3049_obs_t.png       8   \n",
      "3050             ../data/states/state_3050_obs_t.png       8   \n",
      "3051             ../data/states/state_3051_obs_t.png       8   \n",
      "...                                              ...     ...   \n",
      "102619  ../data/states/state_1735246030587_obs_t.png       8   \n",
      "102620  ../data/states/state_1735246030687_obs_t.png       8   \n",
      "102621  ../data/states/state_1735246030786_obs_t.png       8   \n",
      "103117  ../data/states/state_1735246080525_obs_t.png       8   \n",
      "103118  ../data/states/state_1735246080626_obs_t.png       8   \n",
      "\n",
      "                                            next_state   done  \n",
      "271               ../data/states/state_271_obs_tp1.png  False  \n",
      "1749             ../data/states/state_1749_obs_tp1.png  False  \n",
      "3049             ../data/states/state_3049_obs_tp1.png  False  \n",
      "3050             ../data/states/state_3050_obs_tp1.png  False  \n",
      "3051             ../data/states/state_3051_obs_tp1.png  False  \n",
      "...                                                ...    ...  \n",
      "102619  ../data/states/state_1735246030587_obs_tp1.png  False  \n",
      "102620  ../data/states/state_1735246030687_obs_tp1.png  False  \n",
      "102621  ../data/states/state_1735246030786_obs_tp1.png  False  \n",
      "103117  ../data/states/state_1735246080525_obs_tp1.png  False  \n",
      "103118  ../data/states/state_1735246080626_obs_tp1.png  False  \n",
      "\n",
      "[447 rows x 4 columns]\n",
      "Number of invalid actions: 447\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "csv_file = \"../data/dataset.csv\"\n",
    "data = pd.read_csv(csv_file)\n",
    "\n",
    "# Find rows where action is outside the valid range [0, 4]\n",
    "invalid_actions = data[(data[\"action\"] < 0) | (data[\"action\"] > 4)]\n",
    "len(invalid_actions)\n",
    "\n",
    "print(\"Invalid actions:\")\n",
    "print(invalid_actions)\n",
    "\n",
    "print(\"Number of invalid actions:\", len(invalid_actions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[(data[\"action\"] >= 0) & (data[\"action\"] <= 4)]\n",
    "data.to_csv(\"../data/dataset_cleaned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Cleaned Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_bottom(img, crop_height):\n",
    "    \"\"\"\n",
    "    Crop the bottom part of an image.\n",
    "\n",
    "    Args:\n",
    "        img (PIL.Image): Input image.\n",
    "        crop_height (int): The height of the region to crop from the bottom.\n",
    "\n",
    "    Returns:\n",
    "        PIL.Image: Cropped image.\n",
    "    \"\"\"\n",
    "    width, height = img.size  \n",
    "    new_height = height - crop_height\n",
    "    return F.crop(img, top=0, left=0, height=new_height, width=width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_dataset = \"../data/dataset_cleaned.csv\"\n",
    "transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Lambda(lambda img: crop_bottom(img, crop_height=15)),\n",
    "        #transforms.Resize((80, 80)),\n",
    "        transforms.ToTensor(),  # Convert PIL Image to Tensor\n",
    "        # transforms.Normalize( mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5] ),  # Normalize to [-1, 1] for RGB channels\n",
    "        transforms.Normalize(mean=[0.5], std=[0.5]),  # Normalize to [-1, 1] for grayscale\n",
    "    ]\n",
    ")\n",
    "\n",
    "dataset = SasDataset(csv_dataset, transforms, convert_to_grayscale=True, num_stacked_frames=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if actions are in the correct range (0-4):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action min: 0\n",
      "Action max: 4\n"
     ]
    }
   ],
   "source": [
    "print(\"Action min:\", dataset.data[\"action\"].min())\n",
    "print(\"Action max:\", dataset.data[\"action\"].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if images got cropped correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.0..0.8509804].\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load an example image\n",
    "img_path = \"../data/states/state_1735213548910_obs_t.png\"\n",
    "img = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "# Apply the transformation\n",
    "transformed_img = transforms(img)\n",
    "\n",
    "# Display the transformed image\n",
    "plt.imshow(transformed_img.permute(1, 2, 0).numpy())\n",
    "plt.title(\"Transformed Image\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(dataset, TRAIN_RATIO=0.7, VAL_RATIO=0.2, TEST_RATIO=0.1):\n",
    "    \"\"\"\n",
    "    Split PyTorch Dataset into training, validation, and test sets.\n",
    "    Args:\n",
    "        dataset: PyTorch Dataset\n",
    "        TRAIN_RATIO: Float\n",
    "        VAL_RATIO: Float\n",
    "        TEST_RATIO: Float\n",
    "    Returns:\n",
    "        train_dataset: PyTorch Subset\n",
    "        val_dataset: PyTorch Subset\n",
    "        test_dataset: PyTorch Subset\n",
    "    \"\"\"\n",
    "    assert (\n",
    "        abs(TRAIN_RATIO + VAL_RATIO + TEST_RATIO - 1.0) < 1e-6\n",
    "    ), \"Ratios must sum to 1\"\n",
    "\n",
    "    # Calculate sizes for each split\n",
    "    DATASET_SIZE = len(dataset)\n",
    "    train_size = int(TRAIN_RATIO * DATASET_SIZE)\n",
    "    val_size = int(VAL_RATIO * DATASET_SIZE)\n",
    "    test_size = DATASET_SIZE - train_size - val_size  # Remaining samples for test set\n",
    "\n",
    "    # Perform the split\n",
    "    train_dataset, val_dataset, test_dataset = random_split(\n",
    "        dataset,\n",
    "        [train_size, val_size, test_size],\n",
    "        generator=torch.Generator().manual_seed(42),\n",
    "    )\n",
    "\n",
    "    return train_dataset, val_dataset, test_dataset\n",
    "\n",
    "\n",
    "train_set, valid_set, test_set = split_dataset(dataset)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=128, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(valid_set, batch_size=128, shuffle=False, num_workers=4)\n",
    "test_loader = DataLoader(test_set, batch_size=128, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State shape: torch.Size([128, 4, 65, 80])\n",
      "Action shape: torch.Size([128])\n",
      "Next State shape: torch.Size([128, 4, 65, 80])\n",
      "Done shape: torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    state, action, next_state, done = batch\n",
    "    print(\"State shape:\", state.shape)  \n",
    "    print(\"Action shape:\", action.shape)  \n",
    "    print(\"Next State shape:\", next_state.shape)  \n",
    "    print(\"Done shape:\", done.shape) \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = EncoderModel(in_channels=4, input_size=(65, 80)) # 1x4x65x80 (4 stacked grayscale frames of 80x80)\n",
    "inverse_model = InverseModel(encoder_output_dim=400, num_actions=5)\n",
    "embedding_model = EmbeddingModel(action_size=5, embedding_size=50)\n",
    "forward_model = ForwardModel(input_dim=450, output_dim=400)\n",
    "\n",
    "mse_loss = nn.MSELoss()\n",
    "cross_entropy_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    list(encoder.parameters()) +\n",
    "    list(embedding_model.parameters()) +\n",
    "    list(forward_model.parameters()) +\n",
    "    list(inverse_model.parameters()),\n",
    "    lr=0.001\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_combined_model(train_loader, val_loader, num_epochs=10, beta=0.5):\n",
    "    \"\"\"\n",
    "    Train the combined model with forward and inverse losses, and validate on the validation set.\n",
    "\n",
    "    Args:\n",
    "        train_loader (DataLoader): DataLoader for training data.\n",
    "        val_loader (DataLoader): DataLoader for validation data.\n",
    "        num_epochs (int): Number of training epochs.\n",
    "        beta (float): Weight for balancing the two losses.\n",
    "\n",
    "    Returns:\n",
    "        dict: History of training and validation losses and accuracy.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Training on device: {device}\")\n",
    "    encoder.to(device)\n",
    "    embedding_model.to(device)\n",
    "    forward_model.to(device)\n",
    "    inverse_model.to(device)\n",
    "\n",
    "    # Initialize history\n",
    "    history = {\n",
    "        \"train_loss\": [],\n",
    "        \"val_loss\": [],\n",
    "        \"train_accuracy\": [],\n",
    "        \"val_accuracy\": [],\n",
    "    }\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        encoder.train()\n",
    "        embedding_model.train()\n",
    "        forward_model.train()\n",
    "        inverse_model.train()\n",
    "\n",
    "        train_loss = 0.0\n",
    "        total_correct = 0\n",
    "        total_samples = 0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            S_t, a_t, S_t_next, done = batch\n",
    "            S_t, a_t, S_t_next = (\n",
    "                S_t.to(device),\n",
    "                a_t.to(device),\n",
    "                S_t_next.to(device),\n",
    "            )\n",
    "\n",
    "            # Encode states\n",
    "            phi_S_t = encoder(S_t)\n",
    "            phi_S_t_next = encoder(S_t_next)\n",
    "\n",
    "            # Forward Model\n",
    "            a_t_embedded = embedding_model(a_t)\n",
    "            phi_hat_S_t_next = forward_model(phi_S_t, a_t_embedded)\n",
    "            forward_loss = mse_loss(phi_hat_S_t_next, phi_S_t_next)\n",
    "\n",
    "            # Inverse Model\n",
    "            logits = inverse_model(torch.cat([phi_S_t, phi_S_t_next], dim=-1))\n",
    "            inverse_loss = cross_entropy_loss(logits, a_t)\n",
    "\n",
    "            # Total Loss\n",
    "            total_loss = beta * forward_loss + (1 - beta) * inverse_loss\n",
    "            train_loss += total_loss.item()\n",
    "\n",
    "            # Compute accuracy\n",
    "            _, predicted_action = torch.max(logits, dim=1)\n",
    "            total_correct += (predicted_action == a_t).sum().item()\n",
    "            total_samples += a_t.size(0)\n",
    "\n",
    "            # Backward Pass and Optimization\n",
    "            optimizer.zero_grad()\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Compute training accuracy\n",
    "        train_accuracy = total_correct / total_samples\n",
    "\n",
    "        # Validation phase\n",
    "        encoder.eval()\n",
    "        embedding_model.eval()\n",
    "        forward_model.eval()\n",
    "        inverse_model.eval()\n",
    "\n",
    "        val_loss = 0.0\n",
    "        total_correct = 0\n",
    "        total_samples = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                S_t, a_t, S_t_next, done = batch\n",
    "                S_t, a_t, S_t_next = (\n",
    "                    S_t.to(device),\n",
    "                    a_t.to(device),\n",
    "                    S_t_next.to(device),\n",
    "                )\n",
    "\n",
    "                # Encode states\n",
    "                phi_S_t = encoder(S_t)\n",
    "                phi_S_t_next = encoder(S_t_next)\n",
    "\n",
    "                # Forward Model\n",
    "                a_t_embedded = embedding_model(a_t)\n",
    "                phi_hat_S_t_next = forward_model(phi_S_t, a_t_embedded)\n",
    "                forward_loss = mse_loss(phi_hat_S_t_next, phi_S_t_next)\n",
    "\n",
    "                # Inverse Model\n",
    "                logits = inverse_model(torch.cat([phi_S_t, phi_S_t_next], dim=-1))\n",
    "                inverse_loss = cross_entropy_loss(logits, a_t)\n",
    "\n",
    "                # Total Loss\n",
    "                total_loss = beta * forward_loss + (1 - beta) * inverse_loss\n",
    "                val_loss += total_loss.item()\n",
    "\n",
    "                # Compute accuracy\n",
    "                _, predicted_action = torch.max(logits, dim=1)\n",
    "                total_correct += (predicted_action == a_t).sum().item()\n",
    "                total_samples += a_t.size(0)\n",
    "\n",
    "        # Compute validation accuracy\n",
    "        val_accuracy = total_correct / total_samples\n",
    "\n",
    "        # Record metrics\n",
    "        history[\"train_loss\"].append(train_loss / len(train_loader))\n",
    "        history[\"val_loss\"].append(val_loss / len(val_loader))\n",
    "        history[\"train_accuracy\"].append(train_accuracy)\n",
    "        history[\"val_accuracy\"].append(val_accuracy)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch+1}/{num_epochs} | \"\n",
    "            f\"Train Loss: {history['train_loss'][-1]:.4f} | \"\n",
    "            f\"Val Loss: {history['val_loss'][-1]:.4f} | \"\n",
    "            f\"Train Acc: {history['train_accuracy'][-1]:.2%} | \"\n",
    "            f\"Val Acc: {history['val_accuracy'][-1]:.2%}\"\n",
    "        )\n",
    "\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on device: cuda\n",
      "Epoch 1/10, Train Loss: 0.3978, Validation Loss: 0.3497\n",
      "Epoch 2/10, Train Loss: 0.2955, Validation Loss: 0.2681\n",
      "Epoch 3/10, Train Loss: 0.2460, Validation Loss: 0.2420\n",
      "Epoch 4/10, Train Loss: 0.2255, Validation Loss: 0.2341\n",
      "Epoch 5/10, Train Loss: 0.2133, Validation Loss: 0.2238\n",
      "Epoch 6/10, Train Loss: 0.2037, Validation Loss: 0.2226\n",
      "Epoch 7/10, Train Loss: 0.1968, Validation Loss: 0.2188\n",
      "Epoch 8/10, Train Loss: 0.1905, Validation Loss: 0.2176\n",
      "Epoch 9/10, Train Loss: 0.1853, Validation Loss: 0.2189\n",
      "Epoch 10/10, Train Loss: 0.1790, Validation Loss: 0.2248\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "history = train_combined_model(train_loader, num_epochs=10, beta=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_train_history(history):\n",
    "    # Plot the losses\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(history[\"train_loss\"], label=\"Train Loss\")\n",
    "    plt.plot(history[\"val_loss\"], label=\"Validation Loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Loss Over Epochs\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot the accuracies\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(history[\"train_accuracy\"], label=\"Train Accuracy\")\n",
    "    plt.plot(history[\"val_accuracy\"], label=\"Validation Accuracy\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(\"Accuracy Over Epochs\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_train_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models saved successfully!\n"
     ]
    }
   ],
   "source": [
    "torch.save(encoder.state_dict(), \"../model-checkpoints/encoder.pth\")\n",
    "torch.save(embedding_model.state_dict(), \"../model-checkpoints/embedding_model.pth\")\n",
    "torch.save(forward_model.state_dict(), \"../model-checkpoints/forward_model.pth\")\n",
    "torch.save(inverse_model.state_dict(), \"../model-checkpoints/inverse_model.pth\")\n",
    "print(\"Models saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(encoder, forward_model, inverse_model, test_loader, beta=0.5):\n",
    "    \"\"\"\n",
    "    Evaluate the model on the test dataset.\n",
    "\n",
    "    Args:\n",
    "        encoder (nn.Module): Encoder model.\n",
    "        forward_model (nn.Module): Forward model.\n",
    "        inverse_model (nn.Module): Inverse model.\n",
    "        test_loader (DataLoader): DataLoader for the test dataset.\n",
    "        beta (float): Weight for combining forward and inverse losses.\n",
    "\n",
    "    Returns:\n",
    "        dict: Evaluation metrics (average loss, forward loss, inverse loss, accuracy).\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Evaluating on device: {device}\")\n",
    "    \n",
    "    # Set models to evaluation mode\n",
    "    encoder.eval()\n",
    "    forward_model.eval()\n",
    "    inverse_model.eval()\n",
    "\n",
    "    # Move models to the appropriate device\n",
    "    encoder = encoder.to(device)\n",
    "    forward_model = forward_model.to(device)\n",
    "    inverse_model = inverse_model.to(device)\n",
    "\n",
    "    # Loss functions\n",
    "    mse_loss = torch.nn.MSELoss()\n",
    "    cross_entropy_loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    total_forward_loss = 0.0\n",
    "    total_inverse_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for batch in test_loader:\n",
    "            state, action, next_state, done = batch\n",
    "            state, action, next_state = (\n",
    "                state.to(device),\n",
    "                action.to(device),\n",
    "                next_state.to(device),\n",
    "            )\n",
    "\n",
    "            # Encode states\n",
    "            phi_S_t = encoder(state)\n",
    "            phi_S_t_next = encoder(next_state)\n",
    "\n",
    "            # Forward Model: Predict next state embedding\n",
    "            action_embedded = embedding_model(action)\n",
    "            phi_hat_S_t_next = forward_model(phi_S_t, action_embedded)\n",
    "            forward_loss = mse_loss(phi_hat_S_t_next, phi_S_t_next)\n",
    "\n",
    "            # Inverse Model: Predict action\n",
    "            logits = inverse_model(torch.cat([phi_S_t, phi_S_t_next], dim=-1))\n",
    "            inverse_loss = cross_entropy_loss(logits, action)\n",
    "\n",
    "            # Combined loss\n",
    "            loss = beta * forward_loss + (1 - beta) * inverse_loss\n",
    "            total_loss += loss.item()\n",
    "            total_forward_loss += forward_loss.item()\n",
    "            total_inverse_loss += inverse_loss.item()\n",
    "\n",
    "            # Compute accuracy for inverse model\n",
    "            _, predicted_action = torch.max(logits, dim=1)\n",
    "            total_correct += (predicted_action == action).sum().item()\n",
    "            total_samples += action.size(0)\n",
    "\n",
    "    # Compute average losses and accuracy\n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    avg_forward_loss = total_forward_loss / len(test_loader)\n",
    "    avg_inverse_loss = total_inverse_loss / len(test_loader)\n",
    "    accuracy = total_correct / total_samples\n",
    "\n",
    "    return {\n",
    "        \"average_loss\": avg_loss,\n",
    "        \"forward_loss\": avg_forward_loss,\n",
    "        \"inverse_loss\": avg_inverse_loss,\n",
    "        \"accuracy\": accuracy,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_58142/1987970483.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  encoder.load_state_dict(torch.load(\"../model-checkpoints/encoder.pth\"))\n",
      "/tmp/ipykernel_58142/1987970483.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  forward_model.load_state_dict(torch.load(\"../model-checkpoints/forward_model.pth\"))\n",
      "/tmp/ipykernel_58142/1987970483.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  inverse_model.load_state_dict(torch.load(\"../model-checkpoints/inverse_model.pth\"))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = EncoderModel(in_channels=4, input_size=(65, 80))\n",
    "forward_model = ForwardModel(input_dim=450, output_dim=400)\n",
    "inverse_model = InverseModel(encoder_output_dim=400, num_actions=5)\n",
    "\n",
    "encoder.load_state_dict(torch.load(\"../model-checkpoints/encoder.pth\"))\n",
    "forward_model.load_state_dict(torch.load(\"../model-checkpoints/forward_model.pth\"))\n",
    "inverse_model.load_state_dict(torch.load(\"../model-checkpoints/inverse_model.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on device: cuda\n",
      "Test Loss: 0.3719\n",
      "Test Forward Loss: 0.0015\n",
      "Test Inverse Loss: 0.7422\n",
      "Test Accuracy: 70.59%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "metrics = evaluate_model(encoder, forward_model, inverse_model, test_loader, beta=0.5)\n",
    "print(f\"Test Loss: {metrics['average_loss']:.4f}\")\n",
    "print(f\"Test Forward Loss: {metrics['forward_loss']:.4f}\")\n",
    "print(f\"Test Inverse Loss: {metrics['inverse_loss']:.4f}\")\n",
    "print(f\"Test Accuracy: {metrics['accuracy']:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gym-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
